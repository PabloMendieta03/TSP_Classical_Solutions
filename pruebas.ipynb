{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de6158dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset\n",
    "import importlib\n",
    "importlib.reload(dataset)\n",
    "from dataset import DatasetCreation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65ee7a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import to_undirected,add_self_loops\n",
    "from torch_geometric.data import Data\n",
    "import itertools\n",
    "import torch\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06b15d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the data, e.g., in this example, we create 1000 graphs with 10 nodes, 1000 with 15, etc.\n",
    "#We create graphs with varying number of nodes such that the neural network can learn to generalize.\n",
    "#Ideally we want many graphs and with many different number of nodes, but that can create computational problems if we lack the resources\n",
    "#or it the GNN we consider have many parameters.\n",
    "#Also, the bigger the graph, the more time concorde will need for solving (NP-hard problem)\n",
    "\n",
    "data_list = []\n",
    "\n",
    "num_datasets = 100\n",
    "\n",
    "for num_nodes in [5, 10, 12, 15]:\n",
    "  node_coords, distance_matrices,solution_paths, solution_adjacencies, distances = DatasetCreation.create_dataset(num_nodes = num_nodes,  num_datasets = num_datasets)\n",
    "\n",
    "  # Peso para el entrenamiento de la red neuronal, mayor número de nodos mayor peso tiene en el entrenamiento. \n",
    "  num_pos = num_nodes\n",
    "  num_neg = num_nodes**2-num_nodes\n",
    "\n",
    "  weight_pos_class = (num_neg/num_pos)\n",
    "\n",
    "  edge_index = torch.tensor(list(itertools.product(np.arange(num_nodes),np.arange(num_nodes))), dtype=torch.long).T.contiguous()\n",
    "  for i in range(len(node_coords)):\n",
    "      edge_attr = torch.tensor(((distance_matrices[i])).flatten()).float().unsqueeze(1)\n",
    "\n",
    "      x = torch.tensor(node_coords[i]).float()\n",
    "      y = torch.tensor(solution_adjacencies[i].flatten()).float().unsqueeze(1)\n",
    "\n",
    "\n",
    "      data = Data(x=x, edge_index=edge_index, y= y, edge_attr=edge_attr)\n",
    "      data.edge_weight = torch.tensor(((distance_matrices[i])).flatten()).float().unsqueeze(1)\n",
    "      data.true_path = torch.Tensor(solution_paths[i])\n",
    "      data.true_distance = torch.Tensor([distances[i]]).unsqueeze(1)\n",
    "      data.num_nodes = num_nodes\n",
    "      data.pos_class_weight = weight_pos_class\n",
    "      data_list.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05af62d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodos del problema:\n",
      "[[89.93830871582031, 44.39252471923828], [29.364057540893555, 99.97833251953125], [90.78205871582031, 55.61297607421875], [90.71626281738281, 31.229068756103516], [38.70973205566406, 92.18704223632812]]\n",
      "\n",
      "Aristas por donde pasa el tour (1 si para , 0 si no pasa):\n",
      "[[0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0]]\n",
      "\n",
      "Camino más corto\n",
      "[0.0, 2.0, 4.0, 1.0, 3.0]\n",
      "\n",
      "Distancia total del tour:\n",
      "[[192.38340759277344]]\n",
      "\n",
      "Peso para la red neuronal:\n",
      "4.0\n"
     ]
    }
   ],
   "source": [
    "ejemplo = data_list[13]\n",
    "\n",
    "print(\"Nodos del problema:\")\n",
    "print(ejemplo.x.tolist())\n",
    "print(\"\\nAristas por donde pasa el tour (1 si para , 0 si no pasa):\")\n",
    "print(ejemplo.y.tolist())\n",
    "print(\"\\nCamino más corto\")\n",
    "print(ejemplo.true_path.tolist())\n",
    "print(\"\\nDistancia total del tour:\")\n",
    "print(ejemplo.true_distance.tolist())\n",
    "print(\"\\nPeso para la red neuronal:\")\n",
    "print(ejemplo.pos_class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e8d2e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import solutions\n",
    "importlib.reload(dataset)\n",
    "from solutions import SolutionAnalysys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ab4283f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data to train, test, val\n",
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "import random\n",
    "\n",
    "total_length = len(data_list)\n",
    "\n",
    "# Calculate lengths of each part based on percentages\n",
    "train_length = int(total_length * 0.7)\n",
    "val_length = int(total_length * 0.1)\n",
    "\n",
    "shuffled_list = random.sample(data_list, len(data_list))\n",
    "\n",
    "# train, validate, test = np.split(data_list, [int(len(data_list)*0.7), int(len(data_list)*0.8)])\n",
    "train_list = shuffled_list[:train_length]\n",
    "val_list = shuffled_list[train_length:train_length + val_length]\n",
    "test_list = shuffled_list[train_length + val_length:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78169e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "#to train the GNN with graphs of different sizes, we have to create batches with each batch having graphs of the same size.\n",
    "#(Not sure there is another way for this)\n",
    "\n",
    "max_batch_size = 256  # Maximum batch size\n",
    "train_batched_data = []\n",
    "\n",
    "# Sort the datasets based on the number of nodes\n",
    "sorted_datasets = sorted(train_list, key=lambda x: x.num_nodes)\n",
    "\n",
    "for dataset in sorted_datasets:\n",
    "    if not train_batched_data or train_batched_data[-1][-1].num_nodes != dataset.num_nodes:\n",
    "        # If the current batch is empty or the last dataset in the current batch has a different number of nodes,\n",
    "        # start a new batch with the current dataset\n",
    "        train_batched_data.append([dataset])\n",
    "    else:\n",
    "        # If adding the current dataset to the last batch doesn't exceed the maximum batch size,\n",
    "        # add it to the last batch\n",
    "        current_batch = train_batched_data[-1]\n",
    "        if sum(data.num_nodes for data in current_batch) + dataset.num_nodes <= max_batch_size:\n",
    "            current_batch.append(dataset)\n",
    "        else:\n",
    "            # Otherwise, start a new batch with the current dataset\n",
    "            train_batched_data.append([dataset])\n",
    "\n",
    "print(len(train_batched_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919b3620",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_batched_data = []\n",
    "\n",
    "# Sort the datasets based on the number of nodes\n",
    "sorted_datasets = sorted(val_list, key=lambda x: x.num_nodes)\n",
    "\n",
    "for dataset in sorted_datasets:\n",
    "    if not val_batched_data or val_batched_data[-1][-1].num_nodes != dataset.num_nodes:\n",
    "        # If the current batch is empty or the last dataset in the current batch has a different number of nodes,\n",
    "        # start a new batch with the current dataset\n",
    "        val_batched_data.append([dataset])\n",
    "    else:\n",
    "        # If adding the current dataset to the last batch doesn't exceed the maximum batch size,\n",
    "        # add it to the last batch\n",
    "        current_batch = val_batched_data[-1]\n",
    "        if sum(data.num_nodes for data in current_batch) + dataset.num_nodes <= max_batch_size:\n",
    "            current_batch.append(dataset)\n",
    "        else:\n",
    "            # Otherwise, start a new batch with the current dataset\n",
    "            val_batched_data.append([dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71c90ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batched_data = []\n",
    "\n",
    "# Sort the datasets based on the number of nodes\n",
    "sorted_datasets = sorted(test_list, key=lambda x: x.num_nodes)\n",
    "\n",
    "for dataset in sorted_datasets:\n",
    "    if not test_batched_data or test_batched_data[-1][-1].num_nodes != dataset.num_nodes:\n",
    "        # If the current batch is empty or the last dataset in the current batch has a different number of nodes,\n",
    "        # start a new batch with the current dataset\n",
    "        test_batched_data.append([dataset])\n",
    "    else:\n",
    "        # If adding the current dataset to the last batch doesn't exceed the maximum batch size,\n",
    "        # add it to the last batch\n",
    "        current_batch = test_batched_data[-1]\n",
    "        if sum(data.num_nodes for data in current_batch) + dataset.num_nodes <= max_batch_size:\n",
    "            current_batch.append(dataset)\n",
    "        else:\n",
    "            # Otherwise, start a new batch with the current dataset\n",
    "            test_batched_data.append([dataset])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f556289",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the data loaders\n",
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_batched_data, batch_size=None, shuffle=True)\n",
    "val_loader = DataLoader(val_batched_data, batch_size=None, shuffle=True)\n",
    "test_loader = DataLoader(test_batched_data, batch_size=None, shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsp-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
